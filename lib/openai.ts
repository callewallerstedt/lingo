export const OPENAI_MODEL = process.env.OPENAI_MODEL || "gpt-4o-mini";
export const TRANSLATION_MODEL = "gpt-4o-mini";

export async function callOpenAI(messages: Array<{ role: string; content: Array<{ type: "text"; text: string }> }>) {
  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey) {
    throw new Error("Missing OPENAI_API_KEY");
  }

  const response = await fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model: OPENAI_MODEL,
      messages: messages,
      // Add cache busting to ensure fresh responses
      user: `user_${Date.now()}_${Math.random()}`,
    }),
  });

  if (!response.ok) {
    const text = await response.text();
    throw new Error(`OpenAI error: ${response.status} ${text}`);
  }

  const data = await response.json();
  const output = data.choices?.[0]?.message?.content || "";
  return output.trim();
}

export async function callOpenAIForTranslation(messages: Array<{ role: string; content: Array<{ type: "text"; text: string }> }>) {
  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey) {
    throw new Error("Missing OPENAI_API_KEY");
  }

  const response = await fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model: TRANSLATION_MODEL,
      messages: messages,
      temperature: 0, // More consistent and faster responses
      max_tokens: 20, // Even smaller limit for word translations
      // Add cache busting to ensure fresh responses
      user: `user_${Date.now()}_${Math.random()}`,
    }),
  });

  if (!response.ok) {
    const text = await response.text();
    throw new Error(`OpenAI error: ${response.status} ${text}`);
  }

  const data = await response.json();
  const output = data.choices?.[0]?.message?.content || "";
  return output.trim();
}

export async function* callOpenAIStreaming(messages: Array<{ role: string; content: Array<{ type: "text"; text: string }> }>) {
  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey) {
    throw new Error("Missing OPENAI_API_KEY");
  }

  const response = await fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model: OPENAI_MODEL,
      messages: messages,
      stream: true,
      // Add cache busting to ensure fresh responses
      user: `user_${Date.now()}_${Math.random()}`,
    }),
  });

  if (!response.ok) {
    const text = await response.text();
    throw new Error(`OpenAI error: ${response.status} ${text}`);
  }

  const reader = response.body?.getReader();
  if (!reader) {
    throw new Error("No response body");
  }

  const decoder = new TextDecoder();
  let buffer = "";

  try {
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      buffer += decoder.decode(value, { stream: true });
      const lines = buffer.split("\n");
      buffer = lines.pop() || "";

      for (const line of lines) {
        if (line.startsWith("data: ")) {
          const data = line.slice(6);
          if (data === "[DONE]") {
            return;
          }

          try {
            const parsed = JSON.parse(data);
            const content = parsed.choices?.[0]?.delta?.content;
            if (content) {
              yield content;
            }
          } catch (e) {
            // Ignore parsing errors for incomplete chunks
          }
        }
      }
    }
  } finally {
    reader.releaseLock();
  }
}
